# -*- coding: utf-8 -*-
"""Topic interpretation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RSFKCfvBXVGRPbjbc0KyNV7nfLGRQbgZ
"""

pip install requests

import pandas as pd
import re

# warnings.filterwarnings("ignore")
from google.colab import drive

drive.mount('/content/drive')

# Load Excel dataset
file_path = '/content/drive/My Drive/Research/MPhil/WebScraping/Journal/Negative reviews/instagram_negative_neutral_reviews.xlsx'  # Replace with your file path

# Load your data
df = pd.read_excel(file_path)

df.head()

df.shape

# Define the topic keywords
topic_keywords = {
    "Topic 0": ["make", "crash", "account", "data", "work", "story", "try", "even", "post", "reel", "phone", "back", "reinstall", "buffer", "delete", "buffering", "video", "chat", "problem", "day", "wifi"],
    "Topic 1": ["uploaded", "internet", "many", "story", "work", "message", "post", "video", "reel", "account", "still", "get", "buffering", "even", "problem", "data", "go"],
    "Topic 2": ["mobile", "work", "story", "account", "post", "video", "frozen", "get", "upload", "problem", "data", "load"],
    "Topic 3": ["face", "anything", "work", "story", "even", "buffering", "post", "video", "wifi", "frozen", "account", "issue", "problem", "day", "data", "open"],
    "Topic 4": ["time", "feed", "story", "work", "buffering", "post", "video", "account", "get", "back", "problem", "fix", "day", "data", "wifi", "load"],
    "Topic 5": ["time", "mobile", "work", "story", "account", "usage", "video", "post", "even", "bug", "buffering", "use", "get", "frozen", "problem", "data", "wifi", "load"],
    "Topic 6": ["play", "take", "make", "never", "story", "work", "account", "frozen", "video", "reel", "sound", "problem", "day", "data", "load"],
    "Topic 7": ["mobile", "work", "even", "account", "story", "post", "video", "buffering", "use", "try", "issue", "problem", "data", "wifi", "load"],
    "Topic 8": ["play", "work", "even", "story", "account", "frozen", "video", "reel", "post", "get", "watch", "problem", "data", "open", "load"],
    "Topic 9": ["time", "mobile", "feed", "story", "work", "account", "post", "nothing", "video", "even", "use", "upload", "problem", "data", "load"],
    "Topic 10": ["account", "see", "data", "picture", "story", "work", "anymore", "issue", "fix", "mobile", "post", "reel", "buffering", "video", "problem", "day", "need", "wifi", "load"],
    "Topic 11": ["story", "work", "buffering", "post", "video", "account", "keep", "wifi", "even", "problem", "fix", "day", "data", "open", "load"],
    "Topic 12": ["play", "problem", "delete", "story", "work", "account", "post", "frozen", "video", "reel", "buffering", "try", "log", "data", "way", "load"],
    "Topic 13": ["account", "data", "notification", "work", "story", "frozen", "issue", "fix", "play", "mobile", "even", "reel", "internet", "buffering", "video", "problem", "day", "wifi", "load"]
}

import re
import pandas as pd

# Function to extract the top 10 most relevant sentences for each topic based on thumbsUpCount
def extract_relevant_sentences(df, topic_keywords):
    results = {}

    for topic, keywords in topic_keywords.items():
        relevant_sentences = []  # Store tuples of (sentence, thumbsUpCount) for each relevant sentence

        for index, row in df.iterrows():
            review_text = row['content']
            thumbs_up = row['thumbsUpCount']  # Get the thumbs-up count for the review

            # Check if review_text is a string before proceeding
            if isinstance(review_text, str):
                review_text = review_text.lower()  # Convert to lowercase
                sentences = re.split(r'(?<=[.!?]) +', review_text)  # Split into sentences

                for sentence in sentences:
                    keywords_found = [keyword for keyword in keywords if keyword in sentence]

                    # Check if the sentence contains at least 5 unique keywords
                    if len(set(keywords_found)) >= 5:
                        # Append sentence with thumbs-up count
                        relevant_sentences.append((sentence.strip(), thumbs_up))

        # Sort sentences by thumbs-up count in descending order and keep only the top 15
        top_sentences = sorted(relevant_sentences, key=lambda x: x[1], reverse=True)[:15]

        # Store only the sentences in the results dictionary
        results[topic] = [sentence for sentence, thumbs_up in top_sentences]

    return results

relevant_sentences = extract_relevant_sentences(df, topic_keywords)

# Display results
for topic, sentences in relevant_sentences.items():
    print(f"\n{topic} - Top 15 Relevant Sentences:")
    for sentence in sentences:
        print(f"- {sentence}")

# Extract relevant sentences
relevant_sentences = extract_relevant_sentences(df, topic_keywords)

import requests

# Function to get response from Hugging Face's Mistral AI model
def get_mistral_response(api_key, prompt):
    url = "https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 180,
            "temperature": 0.3
        }
    }

    response = requests.post(url, headers=headers, json=data)

    # Check if the response is valid and parse JSON correctly
    if response.status_code == 200:
        generated_text = response.json()[0].get("generated_text", "").strip()
        return generated_text.split("\n", 1)[-1].strip() if "\n" in generated_text else generated_text
    else:
        print(f"Error: {response.status_code}, {response.text}")
        return None

# Your Hugging Face API key
api_key = "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"
# Initialize a list to store each topic's data for the DataFrame
data = []

# Prepare prompts for each topic
for topic, sentences in relevant_sentences.items():
    if sentences:  # Proceed only if there are relevant sentences
        # Join relevant sentences to form a single string
        joined_sentences = " ".join(sentences)

        # User Concerns Prompt
        concern_prompt = f"Identify and summarize the issues, problems or frustrations which users experienced, and identify corresponding app features and functionalities in these user reviews: {joined_sentences[:15]} relevant to the keywords: {', '.join(topic_keywords[topic])}."

        # Get responses
        user_concerns = get_mistral_response(api_key, concern_prompt)

        # Print results
        print(f"\n{topic} - User Concerns:")
        if user_concerns:
            print(user_concerns)
        else:
            print("No user concerns found.")

             # Store results in the list for DataFrame
        data.append({
            "Topic": topic,
            "Topic Keywords": ', '.join(topic_keywords[topic]),
            "Generated User Concerns": user_concerns if user_concerns else "No user concerns found."
        })

# Convert the list of dictionaries into a DataFrame
df_results = pd.DataFrame(data)

# Print or save the DataFrame
df_results.head(10)

pip install textstat

from sentence_transformers import SentenceTransformer, util
from textstat import flesch_reading_ease

# Load pre-trained model for embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to calculate cosine similarity
def calculate_relevance(keywords, generated_text):
    keywords_embedding = model.encode(" ".join(keywords), convert_to_tensor=True)
    text_embedding = model.encode(generated_text, convert_to_tensor=True)
    relevance_score = util.cos_sim(keywords_embedding, text_embedding).item()
    return relevance_score

# Normalize clarity score to range [0, 1]
def normalize_clarity(clarity_score, max_clarity=100):
    return min(clarity_score / max_clarity, 1.0)

# Composite score calculation
def calculate_composite_score(relevance, clarity, relevance_weight=0.7, clarity_weight=0.3, max_clarity=100):
    normalized_clarity = normalize_clarity(clarity, max_clarity)
    return (relevance * relevance_weight) + (normalized_clarity * clarity_weight)

# Add relevance, clarity, and combined scores to the DataFrame
scored_data = []
for index, row in df_results.iterrows():
    topic_keywords = row['Topic Keywords'].split(", ")
    generated_text = row['Generated User Concerns']

    if generated_text != "No user concerns found.":
        relevance = calculate_relevance(topic_keywords, generated_text)
        clarity = flesch_reading_ease(generated_text)
        normalized_clarity = normalize_clarity(clarity)
        composite_score = calculate_composite_score(relevance, clarity)

        scored_data.append({
            "Relevance Score": relevance,
            "Normalized Clarity Score": normalized_clarity,
            "Combined Score": composite_score
        })
    else:
        scored_data.append({
            "Relevance Score": 0.0,
            "Normalized Clarity Score": 0.0,
            "Combined Score": 0.0
        })

# Add new columns to the existing DataFrame
scores_df = pd.DataFrame(scored_data)
df_results = pd.concat([df_results, scores_df], axis=1)

df_results.head()

average_combined_score = df_results['Combined Score'].mean()
print(f"Average Combined Score: {average_combined_score}")

df_results.iloc[2,2]

# Save the relevant reviews to a new CSV file
df_results.to_excel('/content/drive/My Drive/Research/MPhil/WebScraping/Journal/Integrating/Topic interpretation/temperature 0.4/instagram_data.xlsx', index=False)

